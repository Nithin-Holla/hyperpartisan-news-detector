#!/bin/bash
#SBATCH --job-name=hyper_best
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=3
#SBATCH --ntasks-per-node=1
#SBATCH --time=08:00:00
#SBATCH --mem=60000M
#SBATCH --partition=gpu_shared_course
#SBATCH --gres=gpu:1
module purge
module load eb
module load Python/3.6.3-foss-2017b
module load cuDNN/7.0.5-CUDA-9.0.176
module load NCCL/2.0.5-CUDA-9.0.176
export LD_LIBRARY_PATH=/hpc/eb/Debian9/cuDNN/7.1-CUDA-8.0.44-GCCcore-5.4.0/lib64:$LD_LIBRARY_PATH

for hyp_batch_size in 16
do
    for lr in 0.01    
    do
        for out_dropout in 0
        do
            for current_run_seed in 3 7 42 100 114
            do
                srun python3 -u train.py --hyperpartisan_batch_size $hyp_batch_size --metaphor_batch_size 64 --learning_rate $lr --output_dropout_rate $out_dropout --deterministic $current_run_seed --model_checkpoint 'checkpoints/hyperpartisan_best_'$current_run_seed'.pt' --vector_file_name 'glove.840B.300d.txt' --vector_cache_dir '.word_vectors_cache' --metaphor_dataset_folder 'data/vua-sequence' --hyperpartisan_dataset_folder 'Hyperpartisan' --max_epochs 15 --hidden_dim 128 --mode Hyperpatisan >> 'output/hyperpartisan_best'$current_run_seed'.out'
            done
        done
    done
done
